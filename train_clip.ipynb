{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "wdO1WXQ_hMwO"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GVUFpMSi3GZn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n5tDhsUd3RpU"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    def __init__(self , image_size = 224, in_channels = 3 , patch_size = 16 , d_model = 768 , n_heads = 12, n_blocks = 12 ,  eps = 1e-6 , dropout = 0.1):\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_blocks = n_blocks\n",
        "        self.eps = eps\n",
        "        self.dropout = dropout\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_patches = (config.image_size // config.patch_size) ** 2\n",
        "        self.patches = nn.Conv2d(in_channels=config.in_channels,\n",
        "                                 out_channels=config.d_model,\n",
        "                                 kernel_size=(config.patch_size, config.patch_size),\n",
        "                                 stride=(config.patch_size, config.patch_size),\n",
        "                                 padding=\"valid\")\n",
        "        self.pos_emb = nn.Embedding(self.num_patches, config.d_model)\n",
        "\n",
        "    def forward(self, img):\n",
        "        patches = self.patches(img)\n",
        "        patch_emb = patches.flatten(2).transpose(1, 2)\n",
        "        pos_ids = torch.arange(0, self.num_patches).unsqueeze(0).to(img.device)\n",
        "        pos_embeddings = self.pos_emb(pos_ids)\n",
        "        patch_embeddings = patch_emb + pos_embeddings\n",
        "        return patch_embeddings\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    def __init__(self , config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.d_k = config.d_model // config.n_heads\n",
        "        self.scale = self.d_k ** 0.5\n",
        "\n",
        "        self.w_q = nn.Linear(config.d_model , config.d_model)\n",
        "        self.w_k = nn.Linear(config.d_model , config.d_model)\n",
        "        self.w_v = nn.Linear(config.d_model , config.d_model)\n",
        "        self.w_o = nn.Linear(config.d_model , config.d_model)\n",
        "\n",
        "    def forward(self , Q , K , V):\n",
        "\n",
        "        batch_size , n_seq , d_model = Q.size()\n",
        "\n",
        "        Q = self.w_q(Q)\n",
        "        K = self.w_k(K)\n",
        "        V = self.w_v(V)\n",
        "\n",
        "        Q = Q.view(batch_size , n_seq , self.n_heads , self.d_k).transpose(1,2)\n",
        "        K = K.view(batch_size , n_seq , self.n_heads , self.d_k).transpose(1,2)\n",
        "        V = V.view(batch_size , n_seq , self.n_heads , self.d_k).transpose(1,2)\n",
        "\n",
        "        attention_scores = Q @ K.transpose(-2,-1) / self.scale\n",
        "        attention_weights = torch.softmax(attention_scores , dim = -1)\n",
        "        attention_values = attention_weights @ V\n",
        "        attention_values_concat = attention_values.transpose(1,2).contiguous().view(batch_size , n_seq , d_model)\n",
        "\n",
        "        attention_out = self.w_o(attention_values_concat)\n",
        "\n",
        "        return attention_out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self , config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(config.d_model , 4 * config.d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc2 = nn.Linear(4 * config.d_model , config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self , config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm_1 = nn.LayerNorm(config.d_model , eps = config.eps)\n",
        "        self.mha = MHA(config)\n",
        "        self.norm_2 = nn.LayerNorm(config.d_model ,eps = config.eps)\n",
        "        self.ffn = MLP(config)\n",
        "\n",
        "    def forward(self , x ):\n",
        "        residual = x\n",
        "        x = self.norm_1(x)\n",
        "        x = self.mha(x , x , x)\n",
        "        x = residual + x\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm_2(x)\n",
        "        x = self.ffn(x)\n",
        "        out = x + residual\n",
        "\n",
        "        return out\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self , config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embeddings= PatchEmbeddings(config)\n",
        "        self.layers = nn.ModuleList([ViTEncoder(config) for _ in range(config.n_blocks)])\n",
        "\n",
        "    def forward(self , x):\n",
        "        patch_embeddings = self.patch_embeddings(x)\n",
        "        x = patch_embeddings\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hn3GuiCesV4",
        "outputId": "c18a4e55-035e-4d86-e9c1-5f09fef78764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 4.0239\n",
            "Epoch 2/25, Loss: 3.9511\n",
            "Epoch 3/25, Loss: 3.8669\n",
            "Epoch 4/25, Loss: 3.8081\n",
            "Epoch 5/25, Loss: 3.7510\n",
            "Model saved after epoch 5\n",
            "Epoch 6/25, Loss: 3.6834\n",
            "Epoch 7/25, Loss: 3.6203\n",
            "Epoch 8/25, Loss: 3.6087\n",
            "Epoch 9/25, Loss: 3.6009\n",
            "Epoch 10/25, Loss: 3.5638\n",
            "Model saved after epoch 10\n",
            "Epoch 11/25, Loss: 3.5544\n",
            "Epoch 12/25, Loss: 3.5487\n",
            "Epoch 13/25, Loss: 3.5521\n",
            "Epoch 14/25, Loss: 3.5527\n",
            "Epoch 15/25, Loss: 3.5374\n",
            "Model saved after epoch 15\n",
            "Epoch 16/25, Loss: 3.5214\n",
            "Epoch 17/25, Loss: 3.5153\n",
            "Epoch 18/25, Loss: 3.4982\n",
            "Epoch 19/25, Loss: 3.4829\n",
            "Epoch 20/25, Loss: 3.4627\n",
            "Model saved after epoch 20\n",
            "Epoch 21/25, Loss: 3.4138\n",
            "Epoch 22/25, Loss: 3.4081\n",
            "Epoch 23/25, Loss: 3.3377\n",
            "Epoch 24/25, Loss: 3.2974\n",
            "Epoch 25/25, Loss: 3.2713\n",
            "Model saved after epoch 25\n",
            "Final model saved after training\n"
          ]
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((224, 224)),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        caption = item['text']\n",
        "        image = item['image']\n",
        "\n",
        "        image = self.transform(image)\n",
        "\n",
        "        inputs = self.tokenizer(caption, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": inputs['input_ids'].squeeze(),\n",
        "            \"attention_mask\": inputs['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, proj_dim):\n",
        "        super().__init__()\n",
        "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.proj = nn.Linear(768, proj_dim)\n",
        "        self.layer_norm = nn.LayerNorm(proj_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, tokens, attention_mask):\n",
        "        text_emb = self.model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
        "        text_emb = text_emb[:, 0, :]\n",
        "        text_proj = self.layer_norm(self.proj(text_emb))\n",
        "        return text_proj\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, proj_dim):\n",
        "        super().__init__()\n",
        "        vit_config = config()\n",
        "        self.model = ViT(vit_config)\n",
        "        self.proj = nn.Linear(vit_config.d_model, proj_dim)\n",
        "        self.layer_norm = nn.LayerNorm(proj_dim)\n",
        "\n",
        "    def forward(self, img):\n",
        "        image_emb = self.model(img)\n",
        "        image_emb = image_emb[:, 0, :]\n",
        "        img_proj = self.layer_norm(self.proj(image_emb))\n",
        "        return img_proj\n",
        "\n",
        "class CLIP(nn.Module):\n",
        "    def __init__(self, temperature=0.7):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder(256)\n",
        "        self.text_encoder = TextEncoder(256)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, img, tokens, attention_mask):\n",
        "        img_logits = self.image_encoder(img)\n",
        "        text_logits = self.text_encoder(tokens, attention_mask)\n",
        "\n",
        "        img_logits = F.normalize(img_logits, dim=-1)\n",
        "        text_logits = F.normalize(text_logits, dim=-1)\n",
        "\n",
        "        logits = torch.matmul(img_logits, text_logits.t()) / self.temperature\n",
        "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
        "\n",
        "        loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)) / 2\n",
        "        return logits, loss\n",
        "\n",
        "def train(model, dataloader, optimizer, scheduler, device, epochs=25):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = batch['image'].to(device)\n",
        "            tokens = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            _, loss = model(images, tokens, attention_mask)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), f\"new_model_epoch_{epoch+1}.pt\")\n",
        "            print(f\"Model saved after epoch {epoch+1}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"model_final.pt\")\n",
        "    print(\"Final model saved after training\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ds = load_dataset(\"lambdalabs/naruto-blip-captions\")\n",
        "train_dataset = CustomDataset(ds['train'])\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model = CLIP().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=len(dataloader) * 10, eta_min=1e-6)\n",
        "\n",
        "train(model, dataloader, optimizer, scheduler, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CLIP().to(device)\n",
        "model.load_state_dict(torch.load(\"/content/new_model_epoch_25.pt\", map_location=device, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def preprocess_multiple_texts(texts):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        attention_masks.append(inputs['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "def perform_inference(image_path, texts):\n",
        "\n",
        "    image = preprocess_image(image_path)\n",
        "    tokens, attention_mask = preprocess_multiple_texts(texts)\n",
        "    image = image.repeat(tokens.size(0), 1, 1, 1)\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(image, tokens, attention_mask)\n",
        "\n",
        "    similarity_scores = torch.nn.functional.softmax(logits, dim=1)\n",
        "    return similarity_scores\n",
        "\n",
        "image_path = \"/content/images.jpg\"\n",
        "texts = [\"A girl with green hair\", \"A boy with orange hair\"]\n",
        "\n",
        "print(\"Test with multiple texts:\")\n",
        "similarity_scores = perform_inference(image_path, texts)\n",
        "print(f\"Similarity scores: {similarity_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK2VBl-qghmh",
        "outputId": "6564ca1d-49dc-41fc-aef4-c120146e8985"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test with multiple texts:\n",
            "Similarity scores: tensor([[0.6898, 0.3102],\n",
            "        [0.6898, 0.3102]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh0n-FXO5IFu"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}